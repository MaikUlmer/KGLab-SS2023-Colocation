# KGLab-SS2023-Colocation

## Task description
This project is developed in the context of the summer 2023 practical lab on knowledge bases and deals with the semantification of the [co-located property](https://www.wikidata.org/wiki/Wikidata:Property_proposal/colocated_with) between a workshop and conference.  
For the detailed problem description refer to [the original problem description](https://github.com/tholzheim/kgl-exercises/wiki#task-colocation).

The purpose of this project is to consider the [CEUR Workshop Proceedings](https://ceur-ws.org/) series and use text extraction techniques to find patterns such as "Proceedings of the 3rd Wikidata Workshop 2022 **co-located with** the 21st International Semantic Web Conference (ISWC2022)" to identify a workshop co-located with a conference.
Then the corresponding entities should be identified and the property set in Wikidata, optionally creating missing entities.

## Project workflow
The given task can be partitioned into different steps:
* Retrieving CEUR-WS volumes
* Extracting co-located information
* Retrieving conference data
    * Wikidata
    * Dblp
* Matching extracted co-located information to Wikidata entities
* Linking workshops to co-located dblp entity
* Linking + matching Wikidata and dblp conferences
* Using a Neo4j graph to manage both kinds of connections
* Writing the definitive results into Wikidata

## Retrieving CEUR-WS volumes
The CEUR-WS volumes can easily be queried as a JSON response using the provided [restful API](http://cvb.bitplan.com). Querying for both the list of volumes and proceedings yields on one hand the required textual descriptions needed for extracting the co-located patterns and on the other hand useful semantic information, such as the Wikidata URI of the event of a given CEUR-WS proceeding (even if not fully complete).

## Extracting co-located information
The co-located information is usually part of some titles or subtitles of a given volume, where the conference the workshop is co-located with follows after the keyword.
Hence, for the extraction we can use simple regular expressions matching the keyword and returning the string following it.

For this purpose the keywords we have identified are:
* co-located/colocated/collocated with
* hosted by
* affiliated with
* in conjunction with
* ' @ ' (not '@' without surrounding spaces)
* part of
* affiliated with/to
* at

It is more difficult however to extract the projected attributes of the co-located conference from the matched string.
Here we combine a few different approaches, like using the 'loctime' attribute that some volumes have set indicating the time place of the event, using regex to find the short title of the conference based on two general patterns and applying natural language processing (NLP) with [Spacy](https://spacy.io/) to extract the time and place from the matched strings.

## Retrieving conference data
### Wikidata
[Wikidata](https://www.wikidata.org/) is the target platform of this project and a semantic wiki with an underlying knowledge graph structure.
Therefore, we can query the required conference data using Wikidata's SPARQL-endpoint without having to resort to further extraction.

### dblp
[dblp](https://dblp.org/) is a second platform specializing in computer science publications we use to generate additional structural data to later verify the co-location of two Wikidata events or supplement information about events missing in Wikidata.

dblp is partially semantified and also provides a SPARQL endpoint for queries, but unlike Wikidata, this semantification is currently only partial, such that processing the dblp results is necessary to get certain attributes that are of interest.

## Matching extracted co-located information to Wikidata entities
The main idea informing the matching process is to do it incrementally:
Certain keywords are more distinctive than others.
While a string containing 'co-located with' definitely signifies a co-location, only some strings containing 'at' do.
Hence, the previous list of keywords to match gives a priority list.
Instead of handling all the possibly conflicting attributes generated by the different keywords at once and possibly wasting a lot of NLP computation, we handle one keyword at a time, extracting the associated attributes and match the partial list of CEUR-WS extracts against the Wikidata conferences.
Once a workshop has been successfully matched, it can be removed from further consideration and the remaining workshops tried to be matched using the next keyword.

For the matching itself, we use attribute based matching.
While we are most often successful in extracting the short title, we cannot simply only use it to match against Wikidata due to the reason, that short titles are not necessarily unique.
A notable example would be the acronym **ISWC**, which is both used by the *international semantic web conference* as well as the *IEEE international symposium on wearable computers*.

Hence we require two events to match in their acronym such as *ISWC 2022* **and** an additional attribute, such as the month or the country of the event.
Since additionally some short titles may have deviations between CEUR-WS and Wikidata, we also perform a match substituting the short match by a year match plus a fuzzy match of the conference titles.
To decide title similarity in this context we use [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) via its [Scikit-Learn](https://scikit-learn.org/stable/) implementation to generate a vector-space embedding and cosine similarity with a specified threshold value to decide whether two strings refer to the same entity.

## Linking workshops to co-located dblp entity
To check the quality of our matches and to add a source for workshops whose co-located conference is not present in Wikidata, we also **link** the workshops to dblp conferences.
Here linking does not refer to the matching we did previously, but to getting the dblp URI of a given workshop and using patterns in them to guess the URI of the co-located conference.

For more details in this process refer to the READ-ME in the dataloaders folder.

## Linking + matching Wikidata and dblp conferences
A different kind of linking procedure can be performed to link Wikidata conferenecs to dblp conferences in cases where the Wikidata entry contains an explicit link into dblp.

Furthermore we also extend our previous matching algorithm to match Wikidata against dblp, which also requires extracting attributes from the titles of the dblp conferences, as the current results generated by dblp sparql queries does not provide the attributes we are interested in.

## Using a Neo4j graph to manage both kinds of connections
To manage and exploit the entity relationships generated by the matching and linking processes, we input the data into [Neo4j](https://neo4j.com/), a graph database with a convenient custom querying language called cypher, with which we can find patterns and manipulate the data.

In this context we can easily execute a connection heuristic: If there is a pair of Wikidata and dblp conference, which themselves are not connected by a relationship (they were neither matched nor linked) but there are three or more workshops connected to both of them, this indicates that the conferences are the same, and we connect them.

We can then also query the graph to differentiate the results.
These can be evaluated regarding two aspects:

First of is the quality of the result, which is given by how connected the workshop node is. This can be categorized like this:

| Connection | Quality | Human intervention | Reasoning |
| ---------- | ------- | ------------------ | --------- |
| Wikidata + dblp, connected to each other | Definite | Not required | The workshop was successfully connected to a Wikidata conference and to the same dblp conference, which witnesses the match. Thus the co-located attribute can automatically be set. |
| Wikidata + dblp, not connected to each other | Promising | Yes/no decision | There is no guarantee that the linked dblp conference is identical to the Wikidata one due to the missing link. One can however manually identify whether this is the case and automatically set attributes based on this decision. If the decision is no, the Wikidata conference can be discarded and the workshop be demoted to the following case. |
| Only linked with dblp | Promising | Identification of Wikidata presence | Since the Workshop is linked to dblp, we have found the co-located conference. However, it may still be the case, that the conference exists in Wikidata and could not be matched due to missing/incorrect attributes, a different convention for the acronym or similar reasons. Hence, one still has to decide, whether to create a new Wikidata item or not. |
| Only matched to Wikidata | Questionable | Yes/no decision | There is only the Wikidata match without any additional witnesses, so the quality of the solution is expected to be lower than the previous one due to incorrect matches. If we were however to rank based on the amount of human intervention required, this would swap places with category number three. |

The second aspect regards only a few CEUR-WS volumes.
When the Wikidata ID is not known, we cannot automatically set the attribute for these volumes, so we will also only store these.

Based on these two attribute axes, we generate 8 different output files and can proceed with processing the definite results.

## Writing the definitive results into Wikidata
The final step entails setting up a bot for Wikidata using [WikibaseIntegrator](https://github.com/LeMyst/WikibaseIntegrator) to automatically update the co-located attribute using the results we have produced previously. Here we err on the side of caution regarding automatic changes, as incorrectly set data can be very hard to identify after the fact.